{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf379585",
   "metadata": {},
   "source": [
    "# Dissertation code base (Version 1.1)\n",
    "\n",
    "## Title: Natural Language Processing and Security - Scraping\n",
    "\n",
    "### Created by: Samuel William Mason\n",
    "### Student no: 210418060"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b3f6d",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a5eac9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string \n",
    "import timeit\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import twint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1ee588",
   "metadata": {},
   "source": [
    "# Scraping tweets - search for educational tweets from relating indivdiuals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0a0f2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_individual_normal(): # Search for educational tweets from relating indivdiuals\n",
    "    \n",
    "    # Creating list to append scraped tweet data to\n",
    "    twitter_tweets_list = []\n",
    "\n",
    "    # add some tweets with higher education tags, to search for relating tweets\n",
    "    Education_tags = ['University of Oxford', 'University of Cambridge', 'Imperial College London', \n",
    "                      'University of Edinburgh', 'University of Manchester', 'Kingâ€™s College London',\n",
    "                      'London School of Economics', 'University of Bristol', 'The University of Warwick', \n",
    "                      'Newcastle University', 'University of Glasgow']\n",
    "    \n",
    "    for i in range(len(Education_tags)):\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('%s near:\"Leeds\" within:700km since:2017-10-01 until:2022-07-01' % (Education_tags[i])).get_items()):\n",
    "            if i>2000:\n",
    "                break\n",
    "            # Create a list of scraped tweets\n",
    "            twitter_tweets_list.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.replyCount, tweet.media, tweet.lang, tweet.outlinks, tweet.source, 0])\n",
    "    \n",
    "    # Creating a dataframe from the list above\n",
    "    df = pd.DataFrame(twitter_tweets_list, columns=['Datetime', 'Tweet Id', 'Username', 'Text', 'Retweet', 'Like', 'Quote', 'Reply', 'Media', 'Language', 'Outlinks', 'Source', 'Status'])\n",
    "    \n",
    "    # Print dataframe credentials\n",
    "    print(\"\\n Higher education dataset\")\n",
    "    \n",
    "    # Check for duplicates in dataframe\n",
    "    print(\"\\n Total no of duplicate tweets: \" + str(df.duplicated(subset='Tweet Id').sum()) + \"\\n\") # Check for duplicate values\n",
    "\n",
    "    # Drop duplicates in dataframe\n",
    "    df=df.drop_duplicates(subset=['Tweet Id']) # drop duplicate values\n",
    "    \n",
    "    # Print head of dataframe\n",
    "    print(df.head())\n",
    "    \n",
    "    # Final check for NaN values\n",
    "    df.isna().any() # Check for \"NaN\" values\n",
    "    \n",
    "    # Save data to CSV\n",
    "    df.to_csv('non_phishing_tweets_individual.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da63cb31",
   "metadata": {},
   "source": [
    "# Scraping tweets -  search for educational tweets from verified university accounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90a260bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_university_normal(): # Search for educational tweets from universities\n",
    "    \n",
    "    # Creating list to append scraped tweet data to\n",
    "    twitter_tweets_list = []\n",
    "\n",
    "    # add some tweets with higher education tags, to search for relating tweets\n",
    "    Education_tags = ['UniofOxford', 'Cambridge_Uni', 'imperialcollege', 'EdinburghUni', 'OfficialUoM',\n",
    "                      'KingsCollegeLon', 'LSEnews', 'BristolUni', 'warwickuni', 'UniofNewcastle', 'UofGlasgow']\n",
    "    \n",
    "    for i in range(len(Education_tags)):\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:%s since:2017-10-01 until:2022-07-01' % (Education_tags[i])).get_items()):\n",
    "            if i>2000:\n",
    "                break\n",
    "            # Create a list of scraped tweets\n",
    "            twitter_tweets_list.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.replyCount, tweet.media, tweet.lang, tweet.outlinks, tweet.source, 0])\n",
    "    \n",
    "    # Creating a dataframe from the list above\n",
    "    df = pd.DataFrame(twitter_tweets_list, columns=['Datetime', 'Tweet Id', 'Username', 'Text', 'Retweet', 'Like', 'Quote', 'Reply', 'Media', 'Language', 'Outlinks', 'Source', 'Status'])\n",
    "    \n",
    "    # Print dataframe credentials\n",
    "    print(\"\\n Higher education dataset\")\n",
    "    \n",
    "    # Check for duplicates in dataframe\n",
    "    print(\"\\n Total no of duplicate tweets: \" + str(df.duplicated(subset='Tweet Id').sum()) + \"\\n\") # Check for duplicate values\n",
    "\n",
    "    # Drop duplicates in dataframe\n",
    "    df=df.drop_duplicates(subset=['Tweet Id']) # drop duplicate values\n",
    "    \n",
    "    # Print head of dataframe\n",
    "    print(df.head())\n",
    "    \n",
    "    # Final check for NaN values\n",
    "    df.isna().any() # Check for \"NaN\" values\n",
    "    \n",
    "    # Save data to CSV\n",
    "    df.to_csv('non_phishing_tweets_university.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58a07d9",
   "metadata": {},
   "source": [
    "# Scraping tweets - search for educational phishing tweets based upon known phishing langauge and suspicious domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2aba53a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_domain_phishing(): # Search for educational phishing tweets based upon known phishing langauge and suspicious domains\n",
    "    \n",
    "    # Creating list to append scraped tweet data to\n",
    "    twitter_tweets_list = []\n",
    "    \n",
    "    # add some tweets with known education related phishing domains, to idenitfy and quantify those tweets that are illigitamte and containe malicious language\n",
    "    Phishing_tags = ['academia-marcial.cu.ma', 'academia.edu', 'academiacge.com.br', 'academiadederechonotarial.org',\n",
    "'academiadoacucar.com.br', 'academiadovolei.org', 'academiaencore.com', 'academialtacostura.com.ve',\n",
    "'academiapatriciadias.com.br', 'academiasion.com', 'academiatech.club', 'academic.ie',\n",
    "'academichighkundal.com','academiclogo.wademcdonald.com', 'academicmiracles.com', 'academicoptionsireland.com',\n",
    "'academy.prosv.ru', 'academyforgirls.com', 'academymediaworks.com', 'academy.mdbrasil.com.br', 'universitylanguageschool.com',\n",
    "'educationalplanet.cu.ma', 'educationbdinfo.com', 'educationequalityalliance.org.au', 'educationgrantapproval.com',\n",
    "'educationjobbd.com', 'educationpremise.com', 'hilltopschools.com', 'scholarship.ps', 'school-my-class.ru',                    \n",
    "'school.cyfrovychok.ua', 'school37-vlg.ru', 'school6serp.ru', 'school8.kvz.kubannet.ru', 'schoolbellsystems.com',\n",
    "'schoolboyish-apport.000webhostapp.com', 'schoolbricks.in', 'schoolfoodshare.com', 'schoolkutti.com',\n",
    "'schoolnotes.com', 'schooloflyceum.com', 'schoolofmastery.org', 'schoolofselfawareness.com', 'schoolofskills.pro',\n",
    "'schoolscap.com', 'schoolspamers.com', 'schoolstore.co.kr', 'owa-university.website', 'sanfranciscostateuniversity1.godaddysites.com',\n",
    "'universityofthephilippines-onlineservices.weebly.com']\n",
    "    \n",
    "    for i in range(len(Phishing_tags)):\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('university %s' % (Phishing_tags[i])).get_items()):\n",
    "            if i>80:\n",
    "                break\n",
    "            # Create a list of scraped tweets\n",
    "            twitter_tweets_list.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.replyCount, tweet.media, tweet.lang, tweet.outlinks, tweet.source, 1])\n",
    "    \n",
    "    for i in range(len(Phishing_tags)):\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('higher education %s' % (Phishing_tags[i])).get_items()):\n",
    "            if i>80:\n",
    "                break\n",
    "            # Create a list of scraped tweets\n",
    "            twitter_tweets_list.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.replyCount, tweet.media, tweet.lang, tweet.outlinks, tweet.source, 1])\n",
    "    \n",
    "    # Creating a dataframe from the list above\n",
    "    df = pd.DataFrame(twitter_tweets_list, columns=['Datetime', 'Tweet Id', 'Username', 'Text', 'Retweet', 'Like', 'Quote', 'Reply', 'Media', 'Language', 'Outlinks', 'Source', 'Status'])\n",
    "    \n",
    "    # Print dataframe credentials\n",
    "    print(\"\\n Phishing dataset\")\n",
    "    \n",
    "    # Check for duplicates in dataframe\n",
    "    print(\"\\n Total no of duplicate tweets: \" + str(df.duplicated(subset='Tweet Id').sum()) + \"\\n\") # Check for duplicate values\n",
    "    \n",
    "     # Drop duplicates in dataframe\n",
    "    df=df.drop_duplicates(subset=['Tweet Id']) # drop duplicate values\n",
    "    \n",
    "    # Print head of dataframe\n",
    "    print(df.head())\n",
    "    \n",
    "    # Final check for NaN values\n",
    "    df.isna().any() # Check for \"NaN\" values\n",
    "    \n",
    "    # Save data to CSV\n",
    "    df.to_csv('phishing_tweets_domain.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58645d",
   "metadata": {},
   "source": [
    "# Scraping tweets - search for educational phishing tweets based upon known phishing langauge and suspicious links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19485151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_URL_phishing(): # Search for educational phishing tweets based upon known phishing langauge and suspicious links\n",
    "    \n",
    "    # Creating list to append scraped tweet data to\n",
    "    twitter_tweets_list = []\n",
    "    \n",
    "    # add some tweets with known education related phishing URl's, to idenitfy and quantify those tweets that are illigitamte and containe malicious language\n",
    "    Phishing_tags = ['academia-marcial.cu.ma', 'academia.edu', 'academiacge.com.br', 'academiadederechonotarial.org',\n",
    "'academiadoacucar.com.br', 'academiadovolei.org', 'academiaencore.com', 'academialtacostura.com.ve',\n",
    "'academiapatriciadias.com.br', 'academiasion.com', 'academiatech.club', 'academic.ie',\n",
    "'academichighkundal.com','academiclogo.wademcdonald.com', 'academicmiracles.com', 'academicoptionsireland.com',\n",
    "'academy.prosv.ru', 'academyforgirls.com', 'academymediaworks.com', 'academy.mdbrasil.com.br', 'universitylanguageschool.com',\n",
    "'educationalplanet.cu.ma', 'educationbdinfo.com', 'educationequalityalliance.org.au', 'educationgrantapproval.com',\n",
    "'educationjobbd.com', 'educationpremise.com', 'hilltopschools.com', 'scholarship.ps', 'school-my-class.ru',                    \n",
    "'school.cyfrovychok.ua', 'school37-vlg.ru', 'school6serp.ru', 'school8.kvz.kubannet.ru', 'schoolbellsystems.com',\n",
    "'schoolboyish-apport.000webhostapp.com', 'schoolbricks.in', 'schoolfoodshare.com', 'schoolkutti.com',\n",
    "'schoolnotes.com', 'schooloflyceum.com', 'schoolofmastery.org', 'schoolofselfawareness.com', 'schoolofskills.pro',\n",
    "'schoolscap.com', 'schoolspamers.com', 'schoolstore.co.kr', 'http://clickbankuniversity.cu.ma',\n",
    "'https://owa-university.website', 'https://sanfranciscostateuniversity1.godaddysites.com', 'https://login-inc.is-a-student.com',\n",
    "'http://allnineoverseaseducation.com', 'http://confirm-your-account.6eeducation.com', 'http://education.calvaryhospital.org',\n",
    "'http://educationjobbd.com', 'http://ekatvameducation.com']\n",
    "\n",
    "    # Iterate through list of known phishing URL's and add them to our list\n",
    "    #path_to_file = \"phishing-links-ACTIVE-TODAY.txt\"\n",
    "    #a_file = open(path_to_file, \"r\", encoding=\"utf8\")\n",
    "    #for line in a_file:\n",
    "        #stripped_line = line.strip()\n",
    "        #Phishing_tags.append(stripped_line)\n",
    "    #a_file.close()\n",
    "    \n",
    "    for i in range(len(Phishing_tags)):\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('university %s' % (Phishing_tags[i])).get_items()):\n",
    "            if i>80:\n",
    "                break\n",
    "            # Create a list of scraped tweets\n",
    "            twitter_tweets_list.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.replyCount, tweet.media, tweet.lang, tweet.outlinks, tweet.source, 1])\n",
    "    \n",
    "    for i in range(len(Phishing_tags)):\n",
    "        # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "        for i,tweet in enumerate(sntwitter.TwitterSearchScraper('higher education %s' % (Phishing_tags[i])).get_items()):\n",
    "            if i>80:\n",
    "                break\n",
    "            # Create a list of scraped tweets\n",
    "            twitter_tweets_list.append([tweet.date, tweet.id, tweet.user.username, tweet.content, tweet.retweetCount, tweet.likeCount, tweet.quoteCount, tweet.replyCount, tweet.media, tweet.lang, tweet.outlinks, tweet.source, 1])\n",
    "\n",
    "    # Creating a dataframe from the list above\n",
    "    df = pd.DataFrame(twitter_tweets_list, columns=['Datetime', 'Tweet Id', 'Username', 'Text', 'Retweet', 'Like', 'Quote', 'Reply', 'Media', 'Language', 'Outlinks', 'Source', 'Status'])\n",
    "    \n",
    "    # Print dataframe credentials\n",
    "    print(\"\\n Phishing dataset\")\n",
    "    \n",
    "    # Check for duplicates in dataframe\n",
    "    print(\"\\n Total no of duplicate tweets: \" + str(df.duplicated(subset='Tweet Id').sum()) + \"\\n\") # Check for duplicate values\n",
    "    \n",
    "     # Drop duplicates in dataframe\n",
    "    df=df.drop_duplicates(subset=['Tweet Id']) # drop duplicate values\n",
    "    \n",
    "    # Print head of dataframe\n",
    "    print(df.head())\n",
    "    \n",
    "    # Final check for NaN values\n",
    "    df.isna().any() # Check for \"NaN\" values\n",
    "    \n",
    "    # Save data to CSV\n",
    "    df.to_csv('phishing_tweets_url.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2ed3b7",
   "metadata": {},
   "source": [
    "# Pipeline function (Run to scrape all Twitter data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c5ef064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline(): # Run all batch processing of data in one pipeline\n",
    "    \n",
    "    # Start time\n",
    "    start = timeit.default_timer()\n",
    "    \n",
    "    print(\"\\n Scraping commencing \\n\")\n",
    "    \n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    # Scrape normal data\n",
    "    scrape_individual_normal()\n",
    "    # Elapsed\n",
    "    elapsed1 = timeit.default_timer() - start\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    # Scrape normal data\n",
    "    scrape_university_normal()\n",
    "    # Elapsed\n",
    "    elapsed2 = timeit.default_timer() - start\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    # Scrape phishing data\n",
    "    scrape_domain_phishing()\n",
    "    # Elapsed\n",
    "    elapsed3 = timeit.default_timer() - start\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    # Scrape phishing data\n",
    "    scrape_URL_phishing()\n",
    "    # Elapsed\n",
    "    elapsed4 = timeit.default_timer() - start\n",
    "    print(\"-----------------------------------------------------------------------------\")\n",
    "    \n",
    "    print(\"\\n Scraping finished \\n\")\n",
    "\n",
    "    #calculate elasped time\n",
    "    elapsed_final = timeit.default_timer() - start\n",
    "    \n",
    "    # Print timings\n",
    "    print(\"Start time: \" + str(start) + \"Seconds (S)\")\n",
    "    print(\"Time taken for batch 1: \" + str(elapsed1) + \" Seconds (S)\")\n",
    "    print(\"Time taken for batch 2: \" + str(elapsed2) + \" Seconds (S)\")\n",
    "    print(\"Time taken for batch 3: \" + str(elapsed3) + \" Seconds (S)\")\n",
    "    print(\"Time taken for batch 4: \" + str(elapsed4) + \" Seconds (S)\")\n",
    "    print(\"Total elasped time: \" + str(elapsed_final) + \" Seconds (S)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117829f2",
   "metadata": {},
   "source": [
    "# Call pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee6400d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Scraping commencing \n",
      "\n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      " Higher education dataset\n",
      "\n",
      " Total no of duplicate tweets: 17\n",
      "\n",
      "                   Datetime             Tweet Id         Username  \\\n",
      "0 2022-06-29 04:59:30+00:00  1542009841832677376        GuyOxford   \n",
      "1 2022-06-22 07:10:37+00:00  1539506125988581376       CaroBarnes   \n",
      "2 2022-06-22 06:06:21+00:00  1539489950286200833        GuyOxford   \n",
      "3 2022-06-20 15:36:51+00:00  1538908748491673602  utkarsh_amitabh   \n",
      "4 2022-06-19 14:23:53+00:00  1538527995685634054    KathrynMcGurk   \n",
      "\n",
      "                                                Text  Retweet  Like  Quote  \\\n",
      "0  Just posted a photo @ Oriel College, Universit...        0     0      0   \n",
      "1  Box fresh bow tie and bands ready for Encaenia...        0     2      0   \n",
      "2  Just posted a photo @ Exeter College, Universi...        0     0      0   \n",
      "3  Blessed to be sharing the @netcapglobal story ...        1     8      0   \n",
      "4  Excited to be spending this week at the Univer...        0    15      0   \n",
      "\n",
      "   Reply                                              Media Language  \\\n",
      "0      0                                               None       en   \n",
      "1      0                                               None       en   \n",
      "2      0                                               None       en   \n",
      "3      0  [Photo(previewUrl='https://pbs.twimg.com/media...       en   \n",
      "4      0  [Photo(previewUrl='https://pbs.twimg.com/media...       en   \n",
      "\n",
      "                                            Outlinks  \\\n",
      "0  [https://www.instagram.com/p/CfYGvbkI9W7/?igsh...   \n",
      "1  [https://www.instagram.com/p/CfGUMKYNR5i/?igsh...   \n",
      "2  [https://www.instagram.com/p/CfGM1MbICjq/?igsh...   \n",
      "3     [https://www.amazon.in/gp/product/9354794963/]   \n",
      "4                                               None   \n",
      "\n",
      "                                              Source  Status  \n",
      "0  <a href=\"http://instagram.com\" rel=\"nofollow\">...       0  \n",
      "1  <a href=\"http://instagram.com\" rel=\"nofollow\">...       0  \n",
      "2  <a href=\"http://instagram.com\" rel=\"nofollow\">...       0  \n",
      "3  <a href=\"http://twitter.com/download/iphone\" r...       0  \n",
      "4  <a href=\"http://twitter.com/download/iphone\" r...       0  \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      " Higher education dataset\n",
      "\n",
      " Total no of duplicate tweets: 0\n",
      "\n",
      "                   Datetime             Tweet Id     Username  \\\n",
      "0 2022-06-30 17:03:04+00:00  1542554321791098880  UniofOxford   \n",
      "1 2022-06-30 14:31:27+00:00  1542516166635032576  UniofOxford   \n",
      "2 2022-06-30 14:28:45+00:00  1542515487858233350  UniofOxford   \n",
      "3 2022-06-30 14:26:17+00:00  1542514866765701122  UniofOxford   \n",
      "4 2022-06-30 14:23:13+00:00  1542514095932420106  UniofOxford   \n",
      "\n",
      "                                                Text  Retweet  Like  Quote  \\\n",
      "0  Our Open Days have come to an end! ðŸŽ“\\n\\nThank ...        8    50      0   \n",
      "1  How many of the signs could you identify with?...       12    33      4   \n",
      "2  8. You check your social media whenever you st...        2    28      0   \n",
      "3  7. You plan activities with posting about them...        0    11      1   \n",
      "4  6. You say no to opportunities for real-life c...        6    19      0   \n",
      "\n",
      "   Reply                                              Media Language Outlinks  \\\n",
      "0      2  [Photo(previewUrl='https://pbs.twimg.com/media...       en     None   \n",
      "1      4  [Photo(previewUrl='https://pbs.twimg.com/media...       en     None   \n",
      "2      4  [Photo(previewUrl='https://pbs.twimg.com/media...       en     None   \n",
      "3      1  [Photo(previewUrl='https://pbs.twimg.com/media...       en     None   \n",
      "4      3  [Photo(previewUrl='https://pbs.twimg.com/media...       en     None   \n",
      "\n",
      "                                              Source  Status  \n",
      "0  <a href=\"https://sproutsocial.com\" rel=\"nofoll...       0  \n",
      "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       0  \n",
      "2  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       0  \n",
      "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       0  \n",
      "4  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       0  \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      " Phishing dataset\n",
      "\n",
      " Total no of duplicate tweets: 0\n",
      "\n",
      "                   Datetime             Tweet Id         Username  \\\n",
      "0 2022-08-17 06:36:30+00:00  1559791260369424384         sonyjraj   \n",
      "1 2022-08-16 18:19:06+00:00  1559605688027922432    DadsonMichael   \n",
      "2 2022-08-16 14:22:03+00:00  1559546029900455936  redmonds_arthur   \n",
      "3 2022-08-15 16:35:37+00:00  1559217257674317824    AyodeleOtaiku   \n",
      "4 2022-08-15 08:22:50+00:00  1559093245426147329     KresySiberia   \n",
      "\n",
      "                                                Text  Retweet  Like  Quote  \\\n",
      "0  #MacewanAuthor Research article on Hindu Gods ...        0     0      0   \n",
      "1  Michael Dadson,University of British Columbia ...        0     0      0   \n",
      "2  Contemporary Understanding \\n\\nNo excavation a...        0     1      0   \n",
      "3  https://t.co/nV51MNhpWE\\nLate Prof. B.A Oso, D...        0     0      0   \n",
      "4  \"Ruthenia (Lithuania-Rus),\" in Europe: A Liter...        0     3      0   \n",
      "\n",
      "   Reply                                              Media Language  \\\n",
      "0      0  [Photo(previewUrl='https://pbs.twimg.com/media...       en   \n",
      "1      0                                               None       en   \n",
      "2      1                                               None       en   \n",
      "3      0                                               None       en   \n",
      "4      0                                               None       en   \n",
      "\n",
      "                                            Outlinks  \\\n",
      "0  [https://www.academia.edu/17418308/Dr_Sony_Jal...   \n",
      "1   [http://Academia.edu, https://go.shr.lc/3vV7WHp]   \n",
      "2  [https://www.academia.edu/8978823/_Palimpsest_...   \n",
      "3            [https://www.academia.edu/video/jWmPzl]   \n",
      "4  [https://www.academia.edu/26072728/_Ruthenia_L...   \n",
      "\n",
      "                                              Source  Status  \n",
      "0  <a href=\"http://twitter.com/download/iphone\" r...       1  \n",
      "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "2  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "4  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      " Phishing dataset\n",
      "\n",
      " Total no of duplicate tweets: 0\n",
      "\n",
      "                   Datetime             Tweet Id         Username  \\\n",
      "0 2022-08-17 06:36:30+00:00  1559791260369424384         sonyjraj   \n",
      "1 2022-08-16 18:19:06+00:00  1559605688027922432    DadsonMichael   \n",
      "2 2022-08-16 14:22:03+00:00  1559546029900455936  redmonds_arthur   \n",
      "3 2022-08-15 16:35:37+00:00  1559217257674317824    AyodeleOtaiku   \n",
      "4 2022-08-15 08:22:50+00:00  1559093245426147329     KresySiberia   \n",
      "\n",
      "                                                Text  Retweet  Like  Quote  \\\n",
      "0  #MacewanAuthor Research article on Hindu Gods ...        0     0      0   \n",
      "1  Michael Dadson,University of British Columbia ...        0     0      0   \n",
      "2  Contemporary Understanding \\n\\nNo excavation a...        0     1      0   \n",
      "3  https://t.co/nV51MNhpWE\\nLate Prof. B.A Oso, D...        0     0      0   \n",
      "4  \"Ruthenia (Lithuania-Rus),\" in Europe: A Liter...        0     3      0   \n",
      "\n",
      "   Reply                                              Media Language  \\\n",
      "0      0  [Photo(previewUrl='https://pbs.twimg.com/media...       en   \n",
      "1      0                                               None       en   \n",
      "2      1                                               None       en   \n",
      "3      0                                               None       en   \n",
      "4      0                                               None       en   \n",
      "\n",
      "                                            Outlinks  \\\n",
      "0  [https://www.academia.edu/17418308/Dr_Sony_Jal...   \n",
      "1   [http://Academia.edu, https://go.shr.lc/3vV7WHp]   \n",
      "2  [https://www.academia.edu/8978823/_Palimpsest_...   \n",
      "3            [https://www.academia.edu/video/jWmPzl]   \n",
      "4  [https://www.academia.edu/26072728/_Ruthenia_L...   \n",
      "\n",
      "                                              Source  Status  \n",
      "0  <a href=\"http://twitter.com/download/iphone\" r...       1  \n",
      "1  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "2  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "3  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "4  <a href=\"https://mobile.twitter.com\" rel=\"nofo...       1  \n",
      "-----------------------------------------------------------------------------\n",
      "\n",
      " Scraping finished \n",
      "\n",
      "Start time: 4.7886293Seconds (S)\n",
      "Time taken for batch 1: 616.2212922 Seconds (S)\n",
      "Time taken for batch 2: 1129.0075186000001 Seconds (S)\n",
      "Time taken for batch 3: 1246.9641747 Seconds (S)\n",
      "Time taken for batch 4: 1374.1731408 Seconds (S)\n",
      "Total elasped time: 1374.1734596000001 Seconds (S)\n"
     ]
    }
   ],
   "source": [
    "pipeline()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
